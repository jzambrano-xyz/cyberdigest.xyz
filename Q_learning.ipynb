{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXnvSw8yCmpuDhqhHveD8p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jzambrano-xyz/cyberdigest.xyz/blob/main/Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning 101"
      ],
      "metadata": {
        "id": "yNyOB5sDfuY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a simple example of Q-learning in Python. We'll use a very basic environment: a 1D world where the agent must move to the right end to win. This example is simplified to illustrate the concepts clearly.\n",
        "\n",
        "The environment:\n",
        "\n",
        "    A linear world with 5 states (positions) labeled 0 to 4.\n",
        "    The agent starts at position 0.\n",
        "    The goal is to reach position 4.\n",
        "    Actions are moving left (0) or right (1).\n",
        "    A reward of 100 is given for reaching the goal, and a small negative reward (-1) for each step, to encourage efficiency.\n",
        "\n",
        "Explanation of the Code:\n",
        "\n",
        "    Initialization: We initialize the environment parameters and the Q-table, which holds the Q-values for each state-action pair.\n",
        "\n",
        "    Action Selection: The choose_action function selects an action for the current state. It uses ε-greedy policy: with probability ε, it explores by choosing a random action, and with probability 1-ε, it exploits the best known action.\n",
        "\n",
        "    Learning: The update function updates the Q-values in the table using the Bellman equation. It takes the current state, next state, reward, and action to adjust the Q-value towards the expected long-term return.\n",
        "\n",
        "    Training Loop: We run multiple episodes where the agent interacts with the environment. In each episode, it starts from the initial state and makes decisions based on its Q-table, updating the table with each step.\n",
        "\n",
        "    Result: After training, the Q-table represents the learned policy, indicating the best action to take in each state.\n",
        "\n",
        "When you run this code, you'll see the trained Q-table, where each row corresponds to a state and each column to an action. The values represent the expected long-term return for taking that action in that state, guiding the agent to the goal."
      ],
      "metadata": {
        "id": "gVLAC-PKd5gY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRZVDN7HaeUG",
        "outputId": "af4cc854-61b7-4594-9a5b-601a79577099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Q-Table:\n",
            "[[ 10.70787775  19.64      ]\n",
            " [ 10.76919405  34.4       ]\n",
            " [ 19.56592218  59.        ]\n",
            " [ 34.16323447 100.        ]\n",
            " [  0.           0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Environment settings\n",
        "n_states = 5\n",
        "actions = [0, 1]  # 0: move left, 1: move right\n",
        "epsilon = 0.1  # Exploration factor\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.6  # Discount factor\n",
        "n_episodes = 1000\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((n_states, len(actions)))\n",
        "\n",
        "# Function to choose the next action\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(actions)  # Explore\n",
        "    else:\n",
        "        return np.argmax(Q[state, :])  # Exploit best known action\n",
        "\n",
        "# Function to learn the Q-value\n",
        "def update(state, state2, reward, action):\n",
        "    predict = Q[state, action]\n",
        "    target = reward + gamma * np.max(Q[state2, :])\n",
        "    Q[state, action] = Q[state, action] + alpha * (target - predict)\n",
        "\n",
        "# Q-learning algorithm\n",
        "for _ in range(n_episodes):\n",
        "    state = 0\n",
        "    while state < n_states - 1:\n",
        "        action = choose_action(state)\n",
        "        state2 = state + (1 if action == 1 else -1)\n",
        "        state2 = max(0, min(state2, n_states - 1))  # Stay within bounds\n",
        "\n",
        "        # Reward function\n",
        "        reward = 100 if state2 == n_states - 1 else -1\n",
        "\n",
        "        # Update Q-Table\n",
        "        update(state, state2, reward, action)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = state2\n",
        "\n",
        "print(\"Trained Q-Table:\")\n",
        "print(Q)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning: Practical application in the stock market"
      ],
      "metadata": {
        "id": "_c03Wrzmf2DK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a more elaborate Q-learning example using an economic scenario. Imagine we have a simplified stock market environment where an agent decides whether to buy, hold, or sell stocks based on their prices.\n",
        "Scenario Description:\n",
        "\n",
        "    States: The states are a combination of the agent's current portfolio status (e.g., amount of stock owned) and a simplified representation of the market condition (e.g., low, medium, or high prices).\n",
        "\n",
        "    Actions: The agent can choose to buy, hold, or sell stocks.\n",
        "\n",
        "    Rewards: The reward is based on the profit or loss made by the agent's actions. For example, selling at a higher price than the buying price yields a positive reward.\n",
        "\n",
        "    Objective: The agent's goal is to maximize profit over a series of trading days.\n",
        "\n",
        "    *The market conditions and stock prices will be simplified for the sake of the example.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "    States and Actions: The states are defined by the combination of market conditions and portfolio status. Actions include buying, holding, or selling stocks.\n",
        "\n",
        "    Rewards: The reward system is designed to reflect economic incentives - selling at higher market conditions yields more profit.\n",
        "\n",
        "    Market Simulation: simulate_market function randomly determines the market condition to simulate market fluctuation.\n",
        "\n",
        "    Portfolio Update: update_portfolio adjusts the agent's portfolio based on the action taken.\n",
        "\n",
        "    Q-learning: The agent learns the optimal policy over multiple episodes, with the objective to maximize profits by making smart buy/sell decisions.\n",
        "\n",
        "    End Condition: An episode ends when the agent's portfolio is either full or empty, simulating a trading period.\n",
        "\n",
        "This code is a basic simulation and can be expanded with more complex market dynamics and portfolio strategies for a more realistic economic model."
      ],
      "metadata": {
        "id": "NFEc7vMCf8il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Environment settings\n",
        "n_market_conditions = 3  # 0: low, 1: medium, 2: high\n",
        "n_portfolio_status = 3  # 0: no stocks, 1: few stocks, 2: many stocks\n",
        "actions = [0, 1, 2]  # 0: buy, 1: hold, 2: sell\n",
        "epsilon = 0.1\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "n_episodes = 1000\n",
        "\n",
        "# Initialize Q-table\n",
        "n_states = n_market_conditions * n_portfolio_status\n",
        "Q = np.zeros((n_states, len(actions)))\n",
        "\n",
        "# Helper function to get the state index\n",
        "def get_state_index(market_condition, portfolio_status):\n",
        "    return market_condition * n_portfolio_status + portfolio_status\n",
        "\n",
        "# Function to choose the next action\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state, :])\n",
        "\n",
        "# Function to simulate the market (simplified)\n",
        "def simulate_market():\n",
        "    return random.randint(0, n_market_conditions - 1)  # Random market condition\n",
        "\n",
        "# Function to update portfolio status based on action\n",
        "def update_portfolio(portfolio, action, market_condition):\n",
        "    if action == 0:  # Buy\n",
        "        return min(n_portfolio_status - 1, portfolio + 1)\n",
        "    elif action == 2 and portfolio > 0:  # Sell\n",
        "        return max(0, portfolio - 1)\n",
        "    return portfolio  # Hold or no change\n",
        "\n",
        "# Function to calculate reward\n",
        "def calculate_reward(portfolio, action, market_condition):\n",
        "    if action == 2:  # Selling\n",
        "        return market_condition * 10 * portfolio  # Higher reward for selling at high prices\n",
        "    return -1 * portfolio  # Small negative reward for holding or buying\n",
        "\n",
        "# Q-learning algorithm\n",
        "for _ in range(n_episodes):\n",
        "    market_condition = simulate_market()\n",
        "    portfolio_status = 1  # Starting with a few stocks\n",
        "\n",
        "    state = get_state_index(market_condition, portfolio_status)\n",
        "    while True:\n",
        "        action = choose_action(state)\n",
        "        new_market_condition = simulate_market()\n",
        "        new_portfolio_status = update_portfolio(portfolio_status, action, market_condition)\n",
        "        reward = calculate_reward(portfolio_status, action, market_condition)\n",
        "\n",
        "        new_state = get_state_index(new_market_condition, new_portfolio_status)\n",
        "        predict = Q[state, action]\n",
        "        target = reward + gamma * np.max(Q[new_state, :])\n",
        "        Q[state, action] += alpha * (target - predict)\n",
        "\n",
        "        state, portfolio_status, market_condition = new_state, new_portfolio_status, new_market_condition\n",
        "\n",
        "        if new_portfolio_status == 0 or new_portfolio_status == n_portfolio_status - 1:\n",
        "            break  # End episode when portfolio is empty or full\n",
        "\n",
        "print(\"Trained Q-Table:\")\n",
        "print(Q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj8iXEv1gM5Y",
        "outputId": "8c939130-51a9-4b01-bcaf-cba3cca1f6d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Q-Table:\n",
            "[[ 0.          0.          0.        ]\n",
            " [-0.90152291  6.14946011  0.        ]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 0.          0.          0.        ]\n",
            " [-0.86491483  4.50513518 10.        ]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 0.          0.          0.        ]\n",
            " [-0.77123208  4.7731272  20.        ]\n",
            " [ 0.          0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember**: The Q-table in the output of the Q-learning algorithm represents the learned values (Q-values) for each combination of state and action.\n",
        "\n",
        "In the context of this stock market example, each state is a combination of the market condition and the portfolio status, and each action is one of the possible decisions (buy, hold, or sell stocks).\n",
        "\n",
        "Here's a breakdown of what the Q-table represents:\n",
        "\n",
        "1. **Rows**: Each row in the Q-table corresponds to a specific state in the environment. On this example, states are derived from a combination of market conditions (low, medium, high) and portfolio status (no stocks, few stocks, many stocks). This means each row represents a unique scenario in the market with a particular portfolio status.\n",
        "\n",
        "2. **Columns**: Each column corresponds to a possible action that the agent can take in that state. On this example, the actions are buy, hold, and sell.\n",
        "\n",
        "3. **Values (Q-values)**: The values in the table (the Q-values) represent the expected cumulative future reward the agent expects to receive if it takes a certain action in a certain state and then follows the optimal policy thereafter. These values are learned and updated over time through the training process.\n",
        "\n",
        "4. **Interpreting Q-Values**:\n",
        "   - A higher Q-value for a specific state-action pair suggests that taking that action in that state is more favorable for the long-term reward.\n",
        "   - For instance, if the Q-value for the state \"medium market condition, few stocks\" and the action \"sell\" is high, it implies that in that market condition, selling stocks is expected to yield a higher return in the long run.\n",
        "\n",
        "5. **Decision Making**: When the agent encounters a state, it looks at the row in the Q-table corresponding to that state and chooses the action with the highest Q-value (exploitation). This is how the learned policy is applied.\n",
        "\n",
        "In summary, the Q-table is essentially a guide for the agent, telling it what action is likely to be the best in each state based on its past experiences and learning. As the agent continues to learn, these values get refined and ideally converge to the optimal policy for the given environment and reward structure.\n",
        "\n",
        "As a visual aid, here is the initialized Q-table for our stock market example, represented in a tabular format:\n",
        "\n",
        "| States                     | Buy | Hold | Sell |\n",
        "|----------------------------|-----|------|------|\n",
        "| Low Market, No Stocks      | 0.0 | 0.0  | 0.0  |\n",
        "| Low Market, Few Stocks     | 0.0 | 0.0  | 0.0  |\n",
        "| Low Market, Many Stocks    | 0.0 | 0.0  | 0.0  |\n",
        "| Medium Market, No Stocks   | 0.0 | 0.0  | 0.0  |\n",
        "| Medium Market, Few Stocks  | 0.0 | 0.0  | 0.0  |\n",
        "| Medium Market, Many Stocks | 0.0 | 0.0  | 0.0  |\n",
        "| High Market, No Stocks     | 0.0 | 0.0  | 0.0  |\n",
        "| High Market, Few Stocks    | 0.0 | 0.0  | 0.0  |\n",
        "| High Market, Many Stocks   | 0.0 | 0.0  | 0.0  |\n",
        "\n",
        "The table starts with all Q-values initialized to zero. Through the learning process, these values will be updated to reflect the expected cumulative reward of each action in each state. The action that the agent would choose in a given state is the one with the highest Q-value in that state's row."
      ],
      "metadata": {
        "id": "70QyGwFDiDNp"
      }
    }
  ]
}